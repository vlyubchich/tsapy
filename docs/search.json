[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis in Python",
    "section": "",
    "text": "Preface\nThis is a collection of lecture notes on applied time series analysis and forecasting using the programming language Python. Each lecture starts by listing the learning objectives and required reading materials, with additional references in the text. The notes introduce the methods and give a few examples but are less detailed than the reading materials. The notes do not substitute a textbook.\nThe audience is expected to be familiar with Python programming and the following statistical concepts and methods: probability distributions, sampling inference and hypothesis testing, correlation analysis, and regression analysis (including simple and multiple linear regression, mixed-effects models, generalized linear models, and generalized additive models).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Time Series Analysis in Python",
    "section": "Authors",
    "text": "Authors\nThis book, Time Series Analysis in Python, is an adaptation by Vyacheslav Lyubchich.\nIt is based on the original work, Time Series Analysis: Lecture Notes with Examples in R, by V. Lyubchich and Y. R. Gel, available at https://vlyubchich.github.io/tsar/ and licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nThis new, adapted work is also licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Time Series Analysis in Python",
    "section": "Citation",
    "text": "Citation\nLyubchich, V. (2025) Time Series Analysis in Python. Edition 2025-10. https://vlyubchich.github.io/tsapy/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Time Series Analysis in Python",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "l01_regression.html",
    "href": "l01_regression.html",
    "title": "1  Review of Linear Regression",
    "section": "",
    "text": "1.1 Diagnostics for the simple linear regression: residual analysis\nAfter this lecture, you should be competent (again) in assessing violations of various assumptions of linear regression models, particularly assumptions about model residuals, by being able to apply visual assessments and formal statistical tests, and interpret the results and effects of the violations.\nObjectives\nReading materials\nAudio overview\nGiven a simple linear regression (SLR) model \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t} + \\epsilon_{t},\n\\] where \\(Y_{t}\\) is the dependent variable and \\(X_{t}\\) is the regressor (independent, predictor) variable, \\(t = 1,\\dots,n\\), and \\(n\\) is the sample size.\nThe Gauss–Markov theorem\nIf \\(\\epsilon_{t}\\) are uncorrelated random variables with common variance, then of all possible estimators \\(\\beta^{\\ast}_{0}\\) and \\(\\beta^{\\ast}_{1}\\) that are linear functions of \\(Y_{t}\\), the least squares estimators have the smallest variance.\nThus, the ordinary least squares (OLS) assumptions are:\nA basic technique for investigating the aptness of a regression model is based on analyzing the residuals \\(\\epsilon_{t}\\). In a residual analysis, we attempt to assess the validity of the OLS assumptions by examining the estimated residuals \\(\\hat{\\epsilon}_{1}, \\dots, \\hat{\\epsilon}_{n}\\) to see if they satisfy the imposed conditions. If the model is apt, the observed residuals should reflect the assumptions listed above.\nWe perform our diagnostics analysis from a step-by-step verification of each assumption. We start with visual diagnostics, then proceed with formal tests. A lot of useful diagnostic information may be obtained from a residual plot.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#diagnostics-for-the-simple-linear-regression-residual-analysis",
    "href": "l01_regression.html#diagnostics-for-the-simple-linear-regression-residual-analysis",
    "title": "1  Review of Linear Regression",
    "section": "",
    "text": "the residuals \\(\\epsilon_{t}\\) have common variance (\\(\\epsilon_{t}\\) are homoskedastic);\nthe residuals \\(\\epsilon_{t}\\) are uncorrelated;\nto provide prediction intervals (PIs), confidence intervals (CIs), and to test hypotheses about the parameters in our model, we also need to assume that\nthe residuals \\(\\epsilon_{t}\\) are normally distributed (\\(\\epsilon_{t} \\sim N (0, \\sigma^{ 2} )\\)).\n\n\n\n\n\n\n\nNote\n\n\n\nIf the residuals are independent and identically distributed and normal (\\(\\epsilon_{t} \\sim\\) i.i.d. \\(N(0, \\sigma^{2}\\))), then all three above properties are automatically satisfied. In this case, \\(\\epsilon_{t}\\) are not only uncorrelated but are independent. To be independent is a much stronger property than to be uncorrelated.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile a given model may still have useful predictive value even when the OLS assumptions are violated, the confidence intervals, prediction intervals, and \\(p\\)-values associated with the \\(t\\)-statistics will generally be incorrect when the OLS assumptions do not hold.\n\n\n\n\n\n1.1.1 Homoskedasticity\nWe plot the residuals \\(\\hat{\\epsilon}_{t}\\) vs. time, fitted values \\(\\hat{Y}_{t}\\), and predictor values \\(X_t\\). If the assumption of constant variance is satisfied, \\(\\hat{\\epsilon}_{t}\\) fluctuate around the zero mean with more or less constant amplitude and this amplitude does not change with time, fitted values \\(\\hat{Y}_{t}\\), and predictor values \\(X_t\\).\nIf the (linear) model is not appropriate, the mean of the residuals may be non-constant, i.e., not always 0. Figure 1.1 shows an example of a random pattern that we would like the residuals to have (no systematic patterns).\n\n\nCode\nn, m, s = 26, 0.0, 522.0\nx = np.random.normal(loc = m, scale = s, size = n)\nfig, ax = plt.subplots(figsize = (8, 5))\nax.plot(x, marker = 'o', markersize = 4, linewidth = 1)\nax.axhline(0, linestyle = '--', color = 'tab:blue')\nax.set_xlabel('Time')\nax.set_ylabel('Residuals')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.1: A time series plot of ‘ideal’ residuals. These residuals \\(x_t\\) are simulated i.i.d. normal.\n\n\n\n\n\nWhat can we notice in a residual plot?\n\nChange of variability with time indicates heterogeneity of variance of the residuals.\nObvious lack of symmetry (around 0) in the plot suggests a lack of normality or presence of outliers.\nSystematic trends in the residuals suggest correlations between the residuals or inadequateness of the proposed model.\n\nSometimes it is possible to transform the dependent or independent variables to remedy these problems, i.e., to get rid of the correlated residuals or to stabilize the variance (see ?sec-wls and ?sec-gls). Otherwise, we need to change (re-specify) the model.\nA useful technique that can guide us in this process is to plot \\(\\hat{\\epsilon}_{t}\\) vs. \\(\\hat{Y}_{t}\\) and \\(\\hat{\\epsilon}_{t}\\) vs. each predictor \\(X_t\\). Similarly to their time series plot, \\(\\hat{\\epsilon}_{t}\\) should fluctuate around the zero mean with more or less constant amplitude.\n\n\n\n\n\n\nNoteExample: Dishwasher shipments model and patterns in residuals\n\n\n\nFigure 1.2 shows the Python code and residuals of a simple linear regression exploring dishwasher shipments (DISH) and private residential investments (RES) for several years.\nHow different are the patterns in Figure 1.2 from those in Figure 1.1?\n\n\nCode\nD = pd.read_csv(\"data/dish.txt\", sep = \"\\t\")\nif \"YEAR\" in D.columns:\n    D = D.rename(columns = {\"YEAR\": \"Year\"})\n\nmod1 = smf.ols(\"DISH ~ RES\", data = D).fit()\nresid1 = mod1.resid\nfitted1 = mod1.fittedvalues\n\nfig, axes = plt.subplots(1, 3, figsize = (12, 3))\n# Residuals vs time\naxes[0].plot(D[\"Year\"], resid1, marker = 'o', markersize = 4, linewidth = 1)\naxes[0].axhline(0, linestyle = '--', color = 'tab:blue')\naxes[0].set_xlabel('Year')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('A')\n# Residuals vs fitted\naxes[1].scatter(fitted1, resid1, s = 20)\naxes[1].axhline(0, linestyle = '--', color = 'tab:blue')\naxes[1].set_xlabel('Fitted values')\naxes[1].set_ylabel('Residuals')\naxes[1].set_title('B')\n# Residuals vs predictor\naxes[2].scatter(D[\"RES\"], resid1, s = 20)\naxes[2].axhline(0, linestyle = '--', color = 'tab:blue')\naxes[2].set_xlabel('Residential investments (RES)')\naxes[2].set_ylabel('Residuals')\naxes[2].set_title('C')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.2: Estimated residuals plotted vs. time, fitted values, and predictor.\n\n\n\n\n\nIn Figure 1.2, we see a pattern of residuals increasing in time, and a pattern of lower variability for high fitted values or high residential investments. Thus, the assumption of homoskedasticity is violated. In Figure 1.1, no such patterns are observed.\n\n\n\n\n1.1.2 Uncorrelatedness\nIt is a deep topic that we shall discuss many times in different variations in the future. When observations are obtained in a time sequence (the topic of time series analysis and our course), there is a high possibility that the errors \\(\\epsilon_{t}\\) are correlated. For instance, if the residual is positive (or negative) for a given day \\(t\\), it is likely that the residual for the following day \\(t+1\\) is also positive (or negative). Such residuals are said to be autocorrelated (i.e., serially correlated). Autocorrelation of many environmental time series is positive.\nWhen the residuals \\(\\epsilon_{t}\\) are related over time, a model for the residuals frequently employed is the first-order autoregressive model, i.e., the AR(1) model.\nThe autoregressive model of the first order, AR(1), is defined as \\[\n\\epsilon_{t} = \\rho \\epsilon_{t - 1} + u_{t},\n\\] where \\(\\rho\\) is the autoregression coefficient (\\(- 1 &lt; \\rho &lt; 1\\)) and \\(u_{t}\\) is an uncorrelated \\(N (0, \\sigma^{2})\\) time series.\nThe model assumes that the residual \\(\\epsilon_{t}\\) at the time \\(t\\) contains a component resulting from the residual \\(\\epsilon_{t - 1}\\) at the time \\(t - 1\\) and a random disturbance \\(u_{t}\\) that is independent of the earlier periods.\nEffects of autocorrelation\nIf the OLS method is employed for the parameter estimation and the residuals \\(\\epsilon_{t}\\) are autocorrelated of the first order, then the consequences are:\n\nThe OLS estimators will still be unbiased, but they no longer have the minimum variance property (see the Gauss–Markov theorem); they tend to be relatively inefficient.\nThe residual mean square error (MSE) can seriously underestimate the true variance of the error terms in the model.\nStandard procedures for CI, PI, and tests using the \\(F\\) and Student’s \\(t\\) distributions are no longer strictly applicable.\n\nFor example, see Section 5.2 in Chatterjee and Simonoff (2013) for more details.\nDurbin–Watson test\nA widely used test for examining whether the residuals in a regression model are correlated is the Durbin–Watson test. This test is based on the AR(1) model for \\(\\epsilon_{t}\\). The one-tail test alternatives are \\[\n\\begin{align}\nH_{0}{:} ~ \\rho = 0 & ~~ vs. ~~ H_{1}{:} ~ \\rho &gt; 0,\\\\\nH_{0}{:} ~ \\rho = 0 & ~~ vs. ~~ H_{1}{:} ~ \\rho &lt; 0,\n\\end{align}\n\\] and the two-tail test is \\[\nH_{0}{:} ~ \\rho = 0 ~~ vs. ~~ H_{1}{:} ~ \\rho \\neq 0.\\\\\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWhen dealing with real data, positive autocorrelation is usually the case.\n\n\nThe Durbin–Watson test statistic DW is based on the differences between the adjacent residuals, \\(\\epsilon_{t} - \\epsilon_{t - 1}\\), and is of the following form: \\[\n\\text{DW} = \\frac{\\sum^{n}_{t = 2} \\left( \\epsilon_{t} - \\epsilon_{t - 1} \\right)^{2}}{\\sum^{n}_{t = 1} \\epsilon^{2}_{t}},\n\\] where \\(\\epsilon_{t}\\) is the regression residual at the time \\(t\\) and \\(n\\) is the number of observations.\nThe DW statistic takes on values in the range \\([0, 4]\\). In fact,\n\nWhen \\(\\epsilon_{t}\\) are positively correlated, adjacent residuals tend to be of similar magnitude so that the numerator of DW will be relatively small or 0.\nWhen \\(\\epsilon_{t}\\) are negatively correlated, adjacent residuals tend to be of similar magnitude but with the opposite sign so that the numerator of DW will be relatively large or equal to 4.\n\nHence, low DW corresponds to positive autocorrelation. Values of DW that tend towards 4 are in the region for negative autocorrelation.\nThe exact action limit for the Durbin–Watson test is difficult to calculate. Hence, the test is used with a lower bound \\(d_{L}\\) and an upper bound \\(d_{U}\\). We may use Table 1.1 as a rule of thumb.\n\n\n\nTable 1.1: Regions of rejection of the null hypothesis for the Durbin–Watson test\n\n\n\n\n\n\n\n\n\n\n\n\nfrom 0 to \\(d_{L}\\)\nfrom \\(d_{L}\\) to \\(d_{U}\\)\nfrom \\(d_{U}\\) to \\(4 - d_{U}\\)\nfrom \\(4 - d_{U}\\) to \\(4 - d_{L}\\)\nfrom \\(4 - d_{L}\\) to 4\n\n\n\n\nReject \\(H_{0}\\), positive autocorrelation\nNeither accept \\(H_{1}\\) or reject \\(H_{0}\\)\nDo not reject \\(H_{0}\\)\nNeither accept \\(H_{1}\\) or reject \\(H_{0}\\)\nReject \\(H_{0}\\), negative autocorrelation\n\n\n\n\n\n\nThe critical values \\(d_{L}\\) and \\(d_{U}\\) have been tabulated for combinations of various sample sizes, significance levels, and number of regressors in a model. For large samples, a normal approximation can be used (Chatterjee and Simonoff 2013): \\[\nz = \\left(\\frac{\\text{DW}}{2} - 1 \\right)\\sqrt{n}.\n\\] Statistical software packages usually provide exact \\(p\\)-values based on the null distribution of the test statistic (a linear combination of \\(\\chi^2\\) variables).\n\n\n\n\n\n\nNoteExample: Dishwasher residuals DW test\n\n\n\nApply the Durbin–Watson test to the residuals from the dishwashers example, i.e., DISH vs. RES, using the Python package statsmodels.\n\ndw_stat = durbin_watson(resid1)\nprint(f\"Durbin-Watson statistic: {dw_stat:.4f}\")\n\n# Approximate p-value calculation\nz = (dw_stat / 2 - 1) * np.sqrt(len(resid1))\np_value_two_sided = 2 * norm.sf(np.abs(z))\n\nprint(f\"Two-sided p-value: {p_value_two_sided:.4f}\")\n\nDurbin-Watson statistic: 0.5714\nTwo-sided p-value: 0.0003\n\n\nBased on the low \\(p\\)-value we can reject the \\(H_{0}\\): \\(\\rho = 0\\) at the 95% confidence level and accept the alternative \\(H_{1}\\): \\(\\rho &gt; 0\\).\n\n\nRuns test\nDepartures of randomness can take so many forms that no single test for randomness is best for all situations. For instance, one of the most common departures from randomness is the tendency of a sequence to persist in its direction of movement.\nWe can count the number of times a sequence of observations crossed a cut-off line, for example, the median line, and use this information to assess the randomness of \\(\\epsilon_t\\). Alternatively, we count successions of positive or negative differences (see Section 1.4 on the difference sign test). Each such succession is called a run.\nThe formal test is the following. When a sequence of \\(N\\) observations with \\(n\\) observations in positive runs and \\(m\\) observations in negative runs is a random process with independent values generated from a continuous distribution, then the sampling distribution of the number of runs \\(R\\) has the mean and variance \\[\n\\mathrm{E}(R) = \\frac{1 + 2nm}{N}, \\qquad \\sigma^2(R) = \\frac{2nm(2nm-n-m)}{N^2(N-1)},\n\\] where \\(N = n + m\\) is the total sample size.\nThe only assumption for this test is that all sample observations come from a continuous distribution.\nThe two-tail alternative is as follows\n\n\\(H_{0}\\): Sequence is generated by a random process;\n\\(H_{1}\\): Sequence is generated by a process containing either persistence or frequent changes in direction.\n\nWhen positive autocorrelation (or persistence) is present, \\(R\\) will be small. On the other hand, if the process involves frequent changes in direction (negative autocorrelation or anti-persistence), \\(R\\) will be too large.\nWhen the number of observations is sufficiently large, i.e., \\(N &gt; 30\\), the runs test statistic \\(R\\) is based on the standardized normal test statistic \\[\nz = \\frac{R - \\mathrm{E}(R)}{ \\sigma(R)}.\n\\] Here \\(z\\) follows approximately a standard normal distribution.\nRuns test is easy to interpret. Runs test allows assessing only the first-order serial correlation in the residuals, i.e., to test whether two residuals that are one lag apart are correlated.\n\n\n\n\n\n\nNoteExample: Dishwasher residuals runs test\n\n\n\n\n# Runs test for simulated data\nz_x, p_x = runstest_1samp(x, correction = False)\nprint(f\"Runs test for simulated data: z = {z_x:.4f}, p-value = {p_x:.4f}\")\n\n# Runs test for residuals\nz_resid, p_resid = runstest_1samp(resid1, correction = False)\nprint(f\"Runs test for residuals: z = {z_resid:.4f}, p-value = {p_resid:.4f}\")\n\nRuns test for simulated data: z = 0.8366, p-value = 0.4028\nRuns test for residuals: z = -2.7468, p-value = 0.0060\n\n\nThe \\(p\\)-value for the runs test for the residuals is very low, which supports the findings of the DW test that residuals are first-order serially correlated.\n\n\n\n\n1.1.3 Normality\nThere are two major ways of checking normality. Graphical methods visualize differences between empirical data and theoretical normal distribution. Numerical methods conduct statistical tests on the null hypothesis that the variable is normally distributed.\nGraphical methods\nGraphical methods visualize the data using graphs, such as histograms, stem-and-leaf plots, box plots, etc. For example, Figure 1.3 shows a histogram of the simulated normally distributed data and the residuals from the dishwasher example with superimposed normal curves with the corresponding mean and standard deviation.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize = (10, 4))\n\n# Histogram for simulated normal values\naxes[0].hist(x, bins = 10, density = True, color = 'grey', alpha = 0.7, edgecolor = 'black')\nx_range = np.linspace(x.min(), x.max(), 100)\naxes[0].plot(x_range, norm.pdf(x_range, loc = np.mean(x), scale = np.std(x, ddof = 1)), \n             color = 'black', linewidth = 2)\naxes[0].set_ylabel('Density')\naxes[0].set_title('Random normal values')\n\n# Histogram for residuals\naxes[1].hist(resid1, bins = 10, density = True, color = 'grey', alpha = 0.7, edgecolor = 'black')\nresid_range = np.linspace(resid1.min(), resid1.max(), 100)\naxes[1].plot(resid_range, norm.pdf(resid_range, loc = np.mean(resid1), scale = np.std(resid1, ddof = 1)), \n             color = 'black', linewidth = 2)\naxes[1].set_ylabel('Density')\naxes[1].set_title('Model residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.3: Histograms of the simulated normally distributed values and estimated regression residuals.\n\n\n\n\n\nAnother very popular graphical method of assessing normality is the quantile-quantile (Q-Q) plot. The Q-Q plot compares the ordered values of a variable with the corresponding ordered values of the normal distribution.\n\n\n\n\n\n\nNote\n\n\n\nQ-Q plots can also be used to compare sample quantiles with quantiles of other, not normal, distribution (e.g., \\(t\\) or gamma distribution), or to compare quantiles of two samples (to assess if both samples come from the same, unspecified, distribution).\n\n\nLet \\(X\\) be a random variable having the property that the equation \\[\n\\Pr \\left( X \\leqslant x \\right) = \\alpha\n\\] has a unique solution \\(x = x_{(\\alpha)}\\) for each \\(0 &lt; \\alpha &lt; 1\\). That is, there exists \\(x_{(\\alpha)}\\) such that \\[\n\\Pr \\left( X \\leqslant x_{(\\alpha)} \\right) = \\alpha\n\\tag{1.1}\\] and no other value of \\(x\\) satisfies Equation 1.1. Then we will call \\(x_{(\\alpha)}\\) the \\(\\alpha\\)th (population) quantile of \\(X\\). Note that any normal distribution has this uniqueness property. If we consider a standard normal \\(Z \\sim N(0, 1)\\), then some well-known quantiles are:\n\n\\(z_{(0.5)} = 0\\) (the median),\n\\(z_{(0.05)} = -1.645\\) and \\(z_{(0.95)} = 1.645\\)\n\\(z_{(0.025)} = -1.96\\) and \\(z_{(0.975)} = 1.96\\)\n\nWe call the 0.25th, 0.5th, 0.75th quantiles the first, the second, and the third quartiles, respectively. The quartiles divide our data into 4 equal parts.\nNow suppose \\(X \\sim N (\\mu, \\sigma^{2})\\). By standardizing to \\(Z \\sim N(0, 1)\\), we obtain \\[\n\\alpha = \\Pr \\left( X \\leqslant x_{(\\alpha)} \\right) = \\Pr \\left( \\frac{X - \\mu}{ \\sigma} \\leqslant \\frac{x_{(\\alpha)} - \\mu}{\\sigma} \\right) = \\Pr \\left( Z \\leqslant \\frac{x_{(\\alpha)} - \\mu}{ \\sigma} \\right) .\n\\]\nWe also have \\(\\alpha = \\Pr (Z \\leqslant z_{(\\alpha)} )\\) by definition. It follows that \\[\nz_{(\\alpha)} = \\frac{x_{(\\alpha)} - \\mu}{ \\sigma} ~~~~ \\mbox{and hence} ~~~~ x_{(\\alpha)} = \\sigma z_{(\\alpha)} + \\mu.\n\\]\nThus, if \\(X\\) is truly normal, a plot of the quantiles of \\(X\\) vs. the quantiles of the standard normal distribution should yield a straight line. A plot of the quantiles of \\(X\\) vs. the quantiles of \\(Z\\) is called a Q-Q plot.\nEstimating quantiles from data\nLet \\(X_{1}, \\dots, X_{n}\\) be a sequence of observations. Ideally, \\(X_{1}, \\dots, X_{n}\\) should represent i.i.d. observations but we will be happy if preliminary tests indicate that they are homoskedastic and uncorrelated (see the previous sections). We order them from the smallest to the largest and indicate this using the notation \\[\nX_{(1/n)} &lt; X_{(2/n)} &lt; X_{(3/n)} &lt; \\dots &lt; X_{\\left((n - 1)/n\\right)} &lt; X_{(n/n)}.\n\\]\nThe above ordering assumes no ties, but ties can be quite common in data, even continuous data, because of rounding. As long as the proportion of ties is small, this method can be used.\nNote that the proportion of observations less than or equal to \\(X_{(k/n)}\\) is exactly \\(k/n\\). Hence \\(X_{(k/n)}\\), called the \\(k\\)th sample quantile, is an estimate of the population quantile \\(x_{(k/n)}\\).\nThe normal Q-Q plot is obtained by plotting the sample quantiles vs. the quantiles of the standard normal distribution. Python’s statsmodels library provides functions to create Q-Q plots with confidence bands.\n\n\n\n\n\n\nNoteExample: Dishwasher residuals normal Q-Q plot\n\n\n\nFigure 1.4 shows the Q-Q plots of the residuals from the dishwasher example and the simulated normal data with the same mean and standard deviation.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize = (10, 4))\n\n# Q-Q plot for simulated normal values\npg.qqplot(x, dist = 'norm', confidence = 0.95, ax = axes[0])\naxes[0].set_title('Random normal values')\naxes[0].set_xlabel('Standard normal quantiles')\naxes[0].set_ylabel('Sample quantiles')\n\n# Q-Q plot for residuals\npg.qqplot(resid1, dist = 'norm', confidence = 0.95, ax = axes[1])\naxes[1].set_title('Model residuals')\naxes[1].set_xlabel('Standard normal quantiles')\naxes[1].set_ylabel('Sample quantiles')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.4: Normal Q-Q plots of the normally distributed simulated values \\(x_t\\) and the dishwasher residuals.\n\n\n\n\n\nBoth Q-Q plots in Figure 1.4 show a good correspondence of the sample quantiles with theoretical normal quantiles, providing no sufficient evidence against normality of the underlying distributions.\n\n\nAlthough visually appealing, these graphical methods do not provide objective criteria to determine the normality of variables.\nShapiro–Wilk normality test\nOne of the most popular numerical methods for assessing normality is the Shapiro–Wilk (SW) test:\n\n\\(H_0\\): the sample data come from a normally distributed population;\n\\(H_1\\): the population is not normally distributed).\n\nThe SW test is the ratio of the best estimator of the variance to the usual corrected sum of squares estimator of the variance. It has been originally constructed by considering the regression of ordered sample values on corresponding expected normal order statistics. The SW statistic is given by \\[\n\\mbox{SW} = \\frac{\\left(\\sum a_{i} x_{(i)} \\right)^{2}}{\\sum \\left(x_{i} - \\bar{x} \\right)^{2}},\n\\] where \\(x_{(i)}\\) are the ordered sample values (\\(x_{(1)}\\) is the smallest) and the \\(a_{i}\\) are constants generated from the means, variances, and covariances of the order statistics of a sample of size \\(n\\) from a normal distribution. The SW statistic lies between 0 and 1. If the SW statistic is close to 1, this indicates the normality of the data. The SW statistic requires the sample size \\(n\\) to be between 7 and 2000.\n\n\n\n\n\n\nNoteExample: Dishwasher residuals normality test\n\n\n\nBased on the \\(p\\)-values below, we cannot reject the null hypothesis of normality in both cases.\n\nstat_x, p_x = shapiro(x)\nprint(f\"Shapiro-Wilk test for simulated data: W = {stat_x:.4f}, p-value = {p_x:.4f}\")\n\nstat_resid, p_resid = shapiro(resid1)\nprint(f\"Shapiro-Wilk test for residuals: W = {stat_resid:.4f}, p-value = {p_resid:.4f}\")\n\nShapiro-Wilk test for simulated data: W = 0.9675, p-value = 0.5609\nShapiro-Wilk test for residuals: W = 0.9750, p-value = 0.7542\n\n\n\n\n\n\n1.1.4 Summary of the simple linear regression residual diagnostics\n\nThe residuals do not have a constant mean.\nThe residuals do not seem to have a constant variance.\nThe residuals are positively correlated.\nThe residuals look normally distributed (but the SW statistic might be affected by the serial correlation of the residuals).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#multiple-linear-regression",
    "href": "l01_regression.html#multiple-linear-regression",
    "title": "1  Review of Linear Regression",
    "section": "1.2 Multiple linear regression",
    "text": "1.2 Multiple linear regression\nHere we consider a case of \\(p\\) explanatory variables \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t,1} + \\dots + \\beta_{p} X_{t,p} + \\epsilon_{t} \\quad (t = 1,\\dots,n).\n\\]\nThis can be expressed more compactly in a matrix notation as \\[\n\\boldsymbol{Y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n\\] where \\(\\boldsymbol{Y} = (Y_{1}, \\dots, Y_{n})^{\\top}\\), \\(\\boldsymbol{\\beta} = (\\beta_{0} , \\dots, \\beta_{p})^{\\top}\\), \\(\\boldsymbol{\\epsilon} = (\\epsilon_{1} , \\dots, \\epsilon_{n})^{\\top}\\); \\(\\boldsymbol{X}\\) is an \\(n \\times (p + 1)\\) design matrix \\[\n\\boldsymbol{X} = \\left(\n\\begin{array}{cccc}\n1 & X_{1,1} & \\dots & X_{1,p} \\\\\n1 & X_{2,1}& \\dots & X_{2,p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & X_{n,1}& \\dots & X_{n,p}\n\\end{array}\n\\right).\n\\]\nHere the historical dataset for the dependent variable consists of the observations \\(Y_{1}, \\dots, Y_{n}\\); the historical dataset for the independent variables consists of the observations in the matrix \\(\\boldsymbol{X}\\).\nMinimizing \\(SSE =(\\boldsymbol{Y} - \\boldsymbol{X} \\hat{\\boldsymbol{\\beta}})^{\\top} (\\boldsymbol{Y} - \\boldsymbol{X} \\hat{\\boldsymbol{\\beta}})\\) yields the least squares solutions \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\top} \\boldsymbol{Y}\n\\] for non-singular \\(\\boldsymbol{X}^{\\top}\\boldsymbol{X}\\).\nThe forecast of a future value \\(Y_{t}\\) is then given by \\[\n\\hat{Y}_{t} = \\boldsymbol{x}^{\\top}_{t} \\hat{\\boldsymbol{\\beta}},\n\\] where \\(\\boldsymbol{x}_{t}\\) is a (column) vector at the time \\(t\\).\nUnder the OLS assumptions (recall them), we obtain \\[\n\\mathrm{var} \\left( \\hat{\\beta}_{j} \\right) = \\sigma^{2} \\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj},\n\\] where the \\(\\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}\\) denotes the \\(j\\)th diagonal element of \\(\\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}\\).\nThis yields \\[\ns.e. \\left( \\hat{\\beta}_{j} \\right) = \\hat{\\sigma} \\sqrt{ \\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}}.\n\\]\nNote that here the degrees of freedom (d.f.) are \\(n - (p + 1) = n - p - 1\\). (The number of estimated parameters for the independent variables is \\(p\\), plus one for the intercept, i.e., \\(p + 1\\).)\nUnder the OLS assumptions, a \\(100(1 - \\alpha)\\)% confidence interval for the parameter \\(\\beta_{j}\\) (\\(j = 0, 1, \\dots, p\\)) is given by \\[\n\\begin{split}\n\\hat{\\beta}_{j} &\\pm t_{\\alpha / 2, n - (p+1)} s.e.\\left( \\hat{\\beta}_{j} \\right) \\text{ or} \\\\\n\\hat{\\beta}_{j} &\\pm t_{\\alpha / 2, n - (p+1)} \\hat{\\sigma} \\sqrt{\\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}}.\n\\end{split}\n\\tag{1.2}\\]\nTypically, \\(s.e.(\\hat{\\beta}_{j})\\) is available directly from the Python output, so Equation 1.2 is calculated automatically.\nUnder the OLS assumptions, it can be shown that \\[\n\\mathrm{var} \\left( Y_{t} - \\hat{Y}_{t} \\right) = \\sigma^{2} \\left( \\boldsymbol{x}^{\\top}_{t} \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{- 1} \\boldsymbol{x}_{t} + 1 \\right),\n\\] yielding a \\(100(1 - \\alpha)\\)% prediction interval for \\(Y_{t}\\): \\[\n\\boldsymbol{x}^{\\top}_{t} \\hat{\\boldsymbol{\\beta}} \\pm t_{\\alpha / 2, n-(p+1)} \\hat{\\sigma} \\sqrt{ \\boldsymbol{x}^{\\top}_{t} \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}\\boldsymbol{x}_{t} + 1}.\n\\]\nWe usually never perform these calculations by hand and will use the corresponding software functions, e.g., using the method get_prediction() in statsmodels, see an example code below.\nWhat else can we get from the regression output?\nAs in SLR, we will look for the \\(t\\)-statistics and \\(p\\)-values to get an idea about the statistical significance of each of the predictors \\(X_{t,1}, X_{t,2}, \\dots, X_{t,p}\\). The confidence intervals constructed above correspond to individual tests of hypothesis about a parameter, i.e., \\(H_{0}\\): \\(\\beta_{j} = 0\\) vs. \\(H_{1}\\): \\(\\beta_{j} \\neq 0\\).\nWe can also make use of the \\(F\\)-test. The \\(F\\)-test considers all parameters (other than the intercept \\(\\beta_{0}\\)) simultaneously, testing \\[\n\\begin{split}\nH_{0}{:} ~ \\beta_{1} &= \\dots = \\beta_{p} = 0 ~~~ \\text{vs.} \\\\\nH_{1}{:} ~ \\beta_{j} &\\neq 0 ~~~ \\mbox{for at least one} ~~~ j \\in \\{1, \\dots, p \\}.\n\\end{split}\n\\]\nFormally, \\(F_{\\rm{obs}} = \\rm{MSR/MSE}\\) (the ratio of the mean square due to regression and the mean square due to stochastic errors).\nWe reject \\(H_{0}\\) when \\(F_{\\rm{obs}}\\) is too large relative to a cut-off point determined by the degrees of freedom of the \\(F\\)-distribution. The \\(p\\)-value for this \\(F\\)-test is provided in the regression output. Rejecting \\(H_{0}\\) is equivalent to stating that the model has some explanatory value within the range of the data set, meaning that changes in at least some of the explanatory \\(X\\)-variables correlate to changes in the average value of \\(Y\\).\nRecall that \\[\n\\begin{split}\n\\mathrm{SST} &= \\sum_{i=1}^n(Y_t-\\overline{Y})^2= \\mathrm{SSR} + \\mathrm{SSE},\\\\\n\\mathrm{SSE} &=\\sum_{t=1}^n(Y_t-\\hat{\\beta}_0 - \\hat{\\beta}_1 X_{t,1}-\\dots- \\hat{\\beta}_p X_{t,p})\n\\end{split}\n\\] and, hence, \\[\n\\rm{SSR}=\\rm{SST}-\\rm{SSE}.\n\\]\nTo conclude that the model has a reasonable fit, however, we would additionally like to see a high \\(R^{2}\\) value, where \\[\nR^{2} = \\rm{SSR/SST}\n\\] is the proportion of the total sum of squares explained by the regression.\nSmall \\(R^{2}\\) means that the stochastic fluctuations around the regression line (or prediction equation) are large, making the prediction task difficult, even though there may be a genuine explanatory relationship between the average value \\(\\mathrm{E}(Y)\\) and some of the \\(X\\)-variables.\nAnother criterion to judge the aptness of the obtained model is the adjusted \\(R^2\\): \\[\nR^2_{adj}=1-\\frac{n-1}{n-p}\\left( 1-R^2 \\right).\n\\]\nUnlike \\(R^2\\) itself, \\(R^2_{adj}\\) need not increase if an arbitrary (even useless) predictor is added to the model because of the correction \\((n-1)/(n-p)\\).\n\n\n\n\n\n\nNote\n\n\n\nThe intercept \\(\\beta_{0}\\) is not included in the \\(F\\)-test because there is no explanatory variable associated with it. In other words, \\(\\beta_{0}\\) does not contribute to the regression part of the model.\n\n\n\n\n\n\n\n\nNoteExample: Dishwasher shipments multiple linear regression\n\n\n\nLet us now extend the SLR model that we considered previously and include another potential predictor, the durable goods expenditures (billion of 1972 dollars). The goal is to build a model to predict the unit factory shipments of dishwashers (DISH) vs. private residential investment (RES) and durable goods expenditures (DUR) using a multiple linear regression model (MLR): \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t,1} + \\beta_{2} X_{t,2} + \\epsilon_t,\n\\] where \\(X_{t,1}\\) is the private residential investment RES; \\(X_{t,2}\\) is the durable goods expenditures DUR.\nNow we apply the OLS method to estimate the coefficients \\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\beta_{2}\\).\n\nmod2 = smf.ols(\"DISH ~ RES + DUR\", data = D).fit()\nprint(mod2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   DISH   R-squared:                       0.877\nModel:                            OLS   Adj. R-squared:                  0.867\nMethod:                 Least Squares   F-statistic:                     82.26\nDate:                Fri, 24 Oct 2025   Prob (F-statistic):           3.31e-11\nTime:                        23:46:05   Log-Likelihood:                -189.63\nNo. Observations:                  26   AIC:                             385.3\nDf Residuals:                      23   BIC:                             389.0\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -1603.0915    391.308     -4.097      0.000   -2412.573    -793.610\nRES           50.9683     11.373      4.482      0.000      27.442      74.495\nDUR           13.7705      2.780      4.954      0.000       8.020      19.521\n==============================================================================\nOmnibus:                        1.852   Durbin-Watson:                   0.413\nProb(Omnibus):                  0.396   Jarque-Bera (JB):                1.075\nSkew:                           0.496   Prob(JB):                        0.584\nKurtosis:                       3.084   Cond. No.                         661.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe have a high \\(R^2_{adj}\\) and both predictors are statistically significant, but can we already wholeheartedly trust these results? We need to perform the residual diagnostics.\nPlot the estimated residuals \\(\\hat{\\epsilon}_{t} = \\hat{Y}_{t} - Y_{t}\\) vs. the observed \\(Y_{t}\\) (\\(t = 1, 2, \\dots, 26\\) or Year) – see Figure 1.5. The plots show a remaining pattern, with residuals peaking, then declining. The assumption of homoskedasticity is violated. We should update the model so that this pattern is modeled or removed.\n\n\nCode\nresid2 = mod2.resid\nfitted2 = mod2.fittedvalues\n\nfig, axes = plt.subplots(1, 2, figsize = (10, 4))\n\n# Residuals vs time\naxes[0].plot(D[\"Year\"], resid2, marker = 'o', markersize = 4, linewidth = 1)\naxes[0].axhline(0, linestyle = '--', color = 'tab:blue')\naxes[0].set_xlabel('Year')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('A')\n\n# Residuals vs fitted\naxes[1].scatter(fitted2, resid2, s = 20)\naxes[1].axhline(0, linestyle = '--', color = 'tab:blue')\naxes[1].set_xlabel('Fitted values')\naxes[1].set_ylabel('Residuals')\naxes[1].set_title('B')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.5: Residuals vs. time and vs. fitted values.\n\n\n\n\n\nCheck that the residuals \\(\\epsilon_{t}\\) are uncorrelated (the Durbin–Watson and the runs tests):\n\ndw_stat2 = durbin_watson(resid2)\nprint(f\"Durbin-Watson statistic: {dw_stat2:.4f}\")\n\n# Approximate p-value calculation\nz = (dw_stat / 2 - 1) * np.sqrt(len(resid1))\np_value_two_sided = 2 * norm.sf(np.abs(z))\n\nprint(f\"Two-sided p-value: {p_value_two_sided:.4f}\")\n\nDurbin-Watson statistic: 0.4128\nTwo-sided p-value: 0.0003\n\n\n\nz_resid2, p_resid2 = runstest_1samp(resid2, correction = False)\nprint(f\"Runs test: z = {z_resid2:.4f}, p-value = {p_resid2:.4f}\")\n\nRuns test: z = -2.7886, p-value = 0.0053\n\n\nBoth the tests reject \\(H_0\\) of no autocorrelation; hence the assumption of uncorrelatedness is violated.\nCheck that the residuals \\(\\epsilon_{t}\\) are normally distributed using the Q-Q plot (Figure 1.6) and Shapiro–Wilk test.\n\nstat_resid2, p_resid2_sw = shapiro(resid2)\nprint(f\"Shapiro-Wilk test: W = {stat_resid2:.4f}, p-value = {p_resid2_sw:.4f}\")\n\nShapiro-Wilk test: W = 0.9792, p-value = 0.8556\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize = (6, 6))\n\npg.qqplot(resid2, dist = 'norm', confidence = 0.95, ax = ax)\nax.set_title('Model residuals')\nax.set_xlabel('Standard normal quantiles')\nax.set_ylabel('Sample quantiles')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.6: Normal Q-Q plot of the multiple regression residuals.\n\n\n\n\n\nFigure 1.6 and \\(p\\)-value of the Shapiro–Wilk test do not provide evidence against the null hypothesis of normality. The assumption of normality is satisfied.\n\n\n\n1.2.1 Summary of the multiple linear regression residual diagnostics\n\nThe \\(R^{2}\\) has been improved.\nWe do not see a visible improvement in terms of the mean and variance of the residuals.\nThe residuals are still positively correlated.\nThe residuals look normally distributed.\n\nEven though not all OLS assumptions are satisfied we shall consider how to predict the future values of \\(Y\\) and to construct the prediction intervals using Python.\nFor example, assume that we need to predict the future unit factory shipments of dishwashers (DISH) based on the private residential investment of 100 billion USD and durable goods expenditures of 150 billion USD.\nSupply new values of independent variables and use the method get_prediction().\n\nnew_data = pd.DataFrame({\"RES\": [100.0], \"DUR\": [150.0]})\npred = mod2.get_prediction(new_data)\nprint(pred.summary_frame(alpha = 0.05))\n\n         mean     mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n0  5559.30799  521.113009    4481.303598    6637.312381   4227.130196   \n\n   obs_ci_upper  \n0   6891.485783",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#conclusion",
    "href": "l01_regression.html#conclusion",
    "title": "1  Review of Linear Regression",
    "section": "1.3 Conclusion",
    "text": "1.3 Conclusion\nWe have recalled the standard assumptions about residuals of linear regression models. Remember that there are some other assumptions (e.g., about linear independence of predictors) that must be verified. Refer to the reading materials for a complete list.\nThe methods we have used to test the homogeneity of residuals included various residual plots. The normality of residuals can be assessed using histograms or Q-Q plots and statistical tests such as the Shapiro–Wilk normality test.\nThe use of time series in regression presents additional ways to assess patterns in the regression residuals. A plot of residuals vs. time is assessed for homogeneity and absence of trends. Less obvious patterns, such as autocorrelation, can be tested with parametric and nonparametric tests, such as the Durbin–Watson and runs tests.\nThe statistical techniques we will learn aim to model or extract as much information from time series (including the autocorrelation of regression residuals) as possible, such that the remaining series are completely random.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "l01_regression.html#sec-diffsign",
    "href": "l01_regression.html#sec-diffsign",
    "title": "1  Review of Linear Regression",
    "section": "1.4 Appendix",
    "text": "1.4 Appendix\nDifference sign test\nThe logic behind the difference sign test is that in a random process, there will be roughly the same number of ups (positive differences between consecutive values, i.e., \\(X_t-X_{t-1}\\)) and downs (negative differences).\nBrockwell and Davis (2002): “The difference-sign test must be used with caution. A set of observations exhibiting a strong cyclic component will pass the difference-sign test for randomness since roughly half of the observations will be points of increase.” We may see (Figure 1.2 and Figure 1.5), it is the case for our residuals, even though we have no cyclic component, but have a rise followed by a decline.\n\nfrom scipy.stats import binomtest\n\n# Simple difference sign test implementation \n# (without normal approximation)\ndef difference_sign_test(data, alt = 'two-sided'):\n# Ensure data is a numpy array for calculations\n    diffs = np.diff(np.asarray(data))\n    \n    # Filter out zero differences\n    diffs_no_zero = diffs[diffs != 0]\n    \n    # Count positive, negative, and zero differences\n    n_pos = np.sum(diffs_no_zero &gt; 0)\n    n_neg = np.sum(diffs_no_zero &lt; 0)\n    n_zero = len(diffs) - len(diffs_no_zero)\n    n_total = len(diffs_no_zero)\n    \n    # Handle edge case where all differences are zero\n    if n_total == 0:\n        print(\"Warning: All differences are zero. Test is not applicable.\")\n        return {\"n_positive\": n_pos, \n                \"n_negative\": n_neg, \n                \"n_zero\": n_zero, \n                \"n_total\": n_total, \n                \"p_value\": np.nan}\n\n    # Under H0, proportion of positive should be ~0.5\n    # Test is performed on the non-zero count\n    result = binomtest(n_pos, n_total, 0.5, alternative = alt)\n    \n    # Return a dictionary of results\n    return {\n        \"n_positive\": n_pos,\n        \"n_negative\": n_neg,\n        \"n_zero\": n_zero,\n        \"n_total\": n_total,\n        \"p_value\": result.pvalue\n    }\n\nResults for simulated values.\n\n\nCode\ndifference_sign_test(x)\n\n\n{'n_positive': 12,\n 'n_negative': 13,\n 'n_zero': 0,\n 'n_total': 25,\n 'p_value': 1.0}\n\n\nFor simple linear regression residuals.\n\n\nCode\ndifference_sign_test(resid1.values)\n\n\n{'n_positive': 13,\n 'n_negative': 12,\n 'n_zero': 0,\n 'n_total': 25,\n 'p_value': 1.0}\n\n\nFor multiple linear regression residuals.\n\n\nCode\ndifference_sign_test(resid2.values)\n\n\n{'n_positive': 12,\n 'n_negative': 13,\n 'n_zero': 0,\n 'n_total': 25,\n 'p_value': 1.0}\n\n\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn. John Wiley & Sons, Hoboken, NJ, USA\n\n\nChatterjee S, Simonoff JS (2013) Handbook of regression analysis. John Wiley & Sons, Hoboken, NJ, USA",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Review of Linear Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brockwell PJ, Davis RA (2002) Introduction to time series and\nforecasting, 2nd edn. Springer, New York, NY, USA\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn.\nJohn Wiley & Sons, Hoboken, NJ, USA\n\n\nChatterjee S, Simonoff JS (2013) Handbook of regression analysis. John\nWiley & Sons, Hoboken, NJ, USA",
    "crumbs": [
      "References"
    ]
  }
]